{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xXTPhZrchZV"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aORnQfZ1lNl7"
   },
   "source": [
    "# HDAT9900: Dissertation (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdcTJpW581Hp"
   },
   "source": [
    "#### Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23811,
     "status": "ok",
     "timestamp": 1564304930028,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "mcAq12QOx-gj",
    "outputId": "45bc99a4-ed13-458f-d6a3-67e3484c17a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22986,
     "status": "ok",
     "timestamp": 1564304930029,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "1y3Oivcqx-_9",
    "outputId": "b107b53a-7e91-4a47-f0a4-2953e30b53b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive\n"
     ]
    }
   ],
   "source": [
    "# Change directory and link to google drive\n",
    "cd drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22245,
     "status": "ok",
     "timestamp": 1564304930030,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "p8e3G1Stx_B0",
    "outputId": "b06ffad9-de3e-4f56-ab79-5c112fd4cc4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive\n"
     ]
    }
   ],
   "source": [
    "cd My Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21671,
     "status": "ok",
     "timestamp": 1564304930030,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "Y17dtIGFx_Dk",
    "outputId": "7b472018-bed7-4f6a-8012-a51b76c601ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/5_Code\n"
     ]
    }
   ],
   "source": [
    "cd 5_Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2857,
     "status": "ok",
     "timestamp": 1564304954283,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "HSY0SyHbyG0k",
    "outputId": "1403b44a-31b6-4326-fa7f-0ee6e0c28915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/  model.png  \u001b[01;34mplan-violation\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mstandardisation\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJX4J85c84Iu"
   },
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3120,
     "status": "ok",
     "timestamp": 1564304957610,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "TJRInNzdlS3k",
    "outputId": "c7d89247-e32d-487c-a5d3-fd8dfe2b0cfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#cnn\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "# rnn\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# combined model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set seed\n",
    "seeds = 0\n",
    "np.random.seed(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2frW3IJ_LNNI"
   },
   "source": [
    "## 2. Build Models\n",
    "\n",
    "Let's load the data that we prepared in part 1. We will also reshape the voxel data from a flattened 7698x4096 to a 3D voxel structure of height x width x depth x channels (16x16x16x1).\n",
    "\n",
    "### 2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6546,
     "status": "ok",
     "timestamp": 1564304961046,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "9o-UaVG2xoLV",
    "outputId": "5e1c9525-0f93-4146-e8e5-633d40ca6808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 unique tokens.\n",
      "CNN train shape: \t (5773, 16, 16, 16, 1)\n",
      "RNN train shape: \t (5773, 5)\n",
      "Label train shape: \t (5773, 18)\n",
      "\n",
      "\n",
      "CNN test shape: \t (1925, 16, 16, 16, 1)\n",
      "RNN test shape: \t (1925, 5)\n",
      "Label test shape: \t (1925, 18)\n"
     ]
    }
   ],
   "source": [
    "# 2.1. load data -------------------------------------------------------------------\n",
    "filenames_y = pd.read_csv('data/processed/dataset2labelsorgansclean.csv')\n",
    "X_cnn = np.load('data/processed/dataset2voxels16.npy')\n",
    "X_rnn = filenames_y['organs']\n",
    "#y_orig = filenames_y['class7']\n",
    "#k=8\n",
    "y_orig = filenames_y['class13'] # actually 18 different organs\n",
    "k=18\n",
    "\n",
    "# 2.1.2. prepare cnn data ------------------------------------------------------------\n",
    "size = 16\n",
    "h, w, d = size, size, size\n",
    "c = 1  # Channels 1 = grey scale, 3 = colour\n",
    "\n",
    "#taking random indices to split the dataset into train and test\n",
    "test_ids = np.random.permutation(X_cnn.shape[0])\n",
    "\n",
    "#splitting data and labels into train and test\n",
    "#keeping last 10 entries for testing, rest for training\n",
    "\n",
    "X_train_cnn = X_cnn[test_ids[:-int(np.ceil(X_cnn.shape[0]*0.25))]]\n",
    "X_test_cnn = X_cnn[test_ids[-int(np.ceil(X_cnn.shape[0]*0.25)):]]\n",
    "\n",
    "X_train_cnn = X_train_cnn.reshape(X_train_cnn.shape[0], h, w, d, c)\n",
    "X_test_cnn = X_test_cnn.reshape(X_test_cnn.shape[0], h, w, d, c)\n",
    "\n",
    "# 2.1.3. prepare rnn data ---------------------------------------------------------\n",
    "\n",
    "MAX_NB_CHARS = 26\n",
    "MAX_SEQUENCE_LENGTH = 5\n",
    "EMBEDDING_DIM = 20\n",
    "\n",
    "# 2.1.3.1 Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_CHARS,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True,\n",
    "                      char_level=True)\n",
    "tokenizer.fit_on_texts(y_orig)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_rnn = tokenizer.texts_to_sequences(y_orig)\n",
    "X_rnn = pad_sequences(X_rnn, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train_rnn = X_rnn[test_ids[:-int(np.ceil(X_cnn.shape[0]*0.25))]]\n",
    "X_test_rnn = X_rnn[test_ids[-int(np.ceil(X_cnn.shape[0]*0.25)):]]\n",
    "\n",
    "# 2.1.4. prepare the labels ------------------------------------------------------\n",
    "y = pd.get_dummies(y_orig).values\n",
    "# last 75/25 split\n",
    "y_train = y[test_ids[:-int(np.ceil(X_cnn.shape[0]*0.25))]]\n",
    "y_test = y[test_ids[-int(np.ceil(X_cnn.shape[0]*0.25)):]]\n",
    "\n",
    "# tags\n",
    "my_tags = sorted([i for i in set(y_orig)])\n",
    "\n",
    "# 2.1.5. checks -----------------------------------------------------------------\n",
    "print(\"CNN train shape: \\t\", X_train_cnn.shape)\n",
    "print(\"RNN train shape: \\t\", X_train_rnn.shape)\n",
    "print(\"Label train shape: \\t\", y_train.shape)\n",
    "print(\"\\n\")\n",
    "print(\"CNN test shape: \\t\", X_test_cnn.shape)\n",
    "print(\"RNN test shape: \\t\", X_test_rnn.shape)\n",
    "print(\"Label test shape: \\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcpa-lMAxoLX"
   },
   "source": [
    "---\n",
    "### 2. 2. 1. 3D Convolutional Neural Network (CNN)\n",
    "\n",
    "Input and Output layers:\n",
    "\n",
    "* One Input layer with dimentions 16, 16, 16, 3\n",
    "* Output layer with dimensions 2\n",
    "\n",
    "Convolutions :\n",
    "* Apply 4 Convolutional layer with increasing order of filter size (standard size : 8, 16, 32, 64) and fixed kernel size = (3, 3, 3)\n",
    "* Apply 2 Max Pooling layers, one after 2nd convolutional layer and one after fourth convolutional layer.\n",
    "\n",
    "MLP architecture:\n",
    "* Batch normalization on convolutional architecture\n",
    "* Dense layers with 2 layers followed by dropout to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZKhqsUMxoLY"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2.2.1. Convolutional Neural Network\n",
    "# ==============================================================================\n",
    "\n",
    "def CNN(X_train, X_test, y_train, y_test, k, my_tags):\n",
    "    \n",
    "    # Hyper parameters ---------------------------------------------------------\n",
    "    max_epochs = 25\n",
    "    batch_size = 128\n",
    "    dropout_rate=0.5\n",
    "\n",
    "    # Optimizers\n",
    "    # from keras.optimizers import SGD\n",
    "    # opt = Adadelta(lr=0.001)\n",
    "    opt = Adam(lr=0.01, decay=0.7)\n",
    "    # opt = SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Model Architecture -------------------------------------------------------\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Convolution layers\n",
    "    model.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu',\\\n",
    "                     input_shape=(h, w, d, c)))\n",
    "    model.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    # Add max pooling to obtain the most informative features\n",
    "    model.add(MaxPool3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    # Convolution layers\n",
    "    model.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    model.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    # Add max pooling to obtain the most informative features\n",
    "    model.add(MaxPool3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    # perform batch normalization on the convolution outputs before\n",
    "    # feeding it to MLP architecture\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # create an MLP architecture with dense layers : 4096 -> 512 -> 10\n",
    "    # add dropouts to avoid over-fitting / perform regularization\n",
    "    model.add(Dense(units=(h*w*d), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=512, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=k, activation='softmax'))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, \\\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n####################### Training Model #############################\")\n",
    "    print(\"Training...\")\n",
    "    history = model.fit(x=X_train, y=y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=max_epochs,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=1)\n",
    "\n",
    "    # ==========================================================================\n",
    "    # 3. Results\n",
    "    # ==========================================================================\n",
    "    print(\"###################################################################\")\n",
    "    print(\"\\nResults:\\n\")\n",
    "    accr = model.evaluate(X_test, y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],\n",
    "                                                                  accr[1]))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['acc'], label='train')\n",
    "    plt.plot(history.history['val_acc'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    print(\"###################################################################\")\n",
    "    \n",
    "    # Make predictions \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # evaluate the model\n",
    "    _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('\\nTrain: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "    print(\"\\n####################################################################\")\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(y_test, y_pred.round(), target_names=my_tags)\n",
    "    #report = classification_report(y_test, y_pred.round())\n",
    "    print(\"\\nClassfication Report for test:\\n\", report)\n",
    "    print(\"\\n####################################################################\")\n",
    "    \n",
    "    return(model, report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2S_jzNnuxoLa"
   },
   "source": [
    "---\n",
    "### Document classification (Recurrent Neural Network)\n",
    "\n",
    "In order to improve the performance of our model, we will also utilise the name of which the clinician named the 3D file (organ). This name will then be fed into the NN to give further information.\n",
    "\n",
    "Help: https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "\n",
    "\n",
    "\"Recurrent Neural Networks (RNNs) adopt the same principle, albeit in an extremely simplified version: they process sequences by iterating through the sequence elements and maintaining a \"state\" containing information relative to what they have seen so far. In effect, RNNs are a type of neural network that has an internal loop (Figure 6.8). The state of the RNN is reset in-between processing two different, independent sequences (e.g. two different IMDB reviews), so we still consider one sequence as a single datapoint, a single input to the network—what changes is that this datapoint is no longer processed in a single step, rather, the network internally loops over sequence elements. \" \n",
    "\n",
    "### LSTM for name classification\n",
    "\n",
    "Based on the filename of the organ named by the clinician, we will create a Recurrent Neural Network (RNN) using the Long Short Term Memory (LSTM) architecture to predict what the organ is. \n",
    "\n",
    "* Vectorize file names by turning each word into either a sequence of integers or into a vector.\n",
    "* Limit the data set to the top 1000 words.\n",
    "* Set the max number of words in each file to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leJkmP9AxoLa"
   },
   "outputs": [],
   "source": [
    "def rnn(X_train, X_test, Y_train, Y_test, k, my_tags):\n",
    "    print(\"Building Document Classifier... \\n\")\n",
    "    # 0. Hyperparameters -------------------------------------------------------\n",
    "    # The maximum number of words to be used\n",
    "    MAX_NB_WORDS = 26\n",
    "\n",
    "    # Max number of words in each file name\n",
    "    MAX_SEQUENCE_LENGTH = 5\n",
    "\n",
    "    # This is fixed.\n",
    "    EMBEDDING_DIM = 20\n",
    "\n",
    "    epochs = 25\n",
    "    batch_size = 32\n",
    "\n",
    "    '''\n",
    "    # 1. Tokenize the data -----------------------------------------------------\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, char_level=True)\n",
    "    tokenizer.fit_on_texts(name_df['synthetic'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(name_df['synthetic'].values)\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Shape of data tensor:', X.shape)\n",
    "\n",
    "    Y = pd.get_dummies(name_df['tags']).values\n",
    "    print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "    # 2. Split the data --------------------------------------------------------\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "    print(X_train.shape,Y_train.shape)\n",
    "    print(X_test.shape,Y_test.shape)\n",
    "\n",
    "    '''\n",
    "    # 3. Build the model -------------------------------------------------------\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # add dense layer to add to cnn\n",
    "    model.add(Dense(k, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                                                 patience=3, min_delta=0.0001)])\n",
    "\n",
    "    # 4. Results ---------------------------------------------------------------\n",
    "    print(\"###################################################################\")\n",
    "    print(\"\\nResults:\\n\")\n",
    "    accr = model.evaluate(X_test, Y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],\n",
    "                                                                  accr[1]))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['acc'], label='train')\n",
    "    plt.plot(history.history['val_acc'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    \n",
    "    # Make predictions \n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred.round(), target_names=my_tags)\n",
    "    print(\"\\nClassfication Report for test:\\n\", report)\n",
    "    print(\"###################################################################\")\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZwniD3bqxoLc"
   },
   "source": [
    "---\n",
    "### Combined (CNN + RNN)\n",
    "\n",
    "We want to create a model that takes the organ structure from the ply for the CNN and add information from the predicted RNN to help the model become more accurate. \n",
    "\n",
    "This model will take the 3D model of the organ and its filename and learn about the two concurrently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AnBWeTE6xoLc"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv3D, MaxPooling3D, Flatten\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model, Sequential\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def standardisation_model(X_train_cnn, X_train_rnn, X_test_cnn, X_test_rnn, y_train, y_test, k, my_tags):\n",
    "    \n",
    "    # Hyperparameters ----------------------------------------------------------\n",
    "    dropout_rate=0.5\n",
    "    epochs=25\n",
    "    batch_size=128\n",
    "    \n",
    "    # First, let's define a vision model using a Sequential model.\n",
    "    # This model will encode an image into a vector.\n",
    "    vision_model = Sequential()\n",
    "    vision_model.add(Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu', input_shape=(h, w, d, c)))\n",
    "    vision_model.add(Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    vision_model.add(MaxPool3D(pool_size=(2, 2, 2)))\n",
    "    vision_model.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    vision_model.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))\n",
    "    vision_model.add(MaxPool3D(pool_size=(2, 2, 2)))\n",
    "    vision_model.add(BatchNormalization())\n",
    "    vision_model.add(Flatten())\n",
    "    vision_model.add(Dense(units=(h*w*d), activation='relu'))\n",
    "    vision_model.add(Dropout(dropout_rate))\n",
    "    vision_model.add(BatchNormalization())\n",
    "    vision_model.add(Dense(units=512, activation='relu'))\n",
    "    vision_model.add(Dropout(dropout_rate))\n",
    "    vision_model.add(BatchNormalization())\n",
    "\n",
    "    # Now let's get a tensor with the output of our vision model:\n",
    "    cnn_inputs = Input(shape=(h, w, d, c))\n",
    "    encoded_image = vision_model(cnn_inputs)\n",
    "\n",
    "    # Next, let's define a language model to encode the filename into a vector.\n",
    "    # Each filename will be at most 20 characters long,\n",
    "    # and we will index words as integers from 1 to 99.\n",
    "    filename_inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_filename = Embedding(input_dim=MAX_NB_CHARS, output_dim=MAX_SEQUENCE_LENGTH, input_length=MAX_SEQUENCE_LENGTH)(filename_inputs)\n",
    "    embedded_filename = SpatialDropout1D(0.2)(embedded_filename)\n",
    "    encoded_filename = LSTM(100)(embedded_filename)\n",
    "\n",
    "    # Let's concatenate the filename vector and the image vector:\n",
    "    merged = keras.layers.concatenate([encoded_filename, encoded_image])\n",
    "\n",
    "    # And let's train a logistic regression over 100 words on top:\n",
    "    output = Dense(k, activation='softmax')(merged)\n",
    "\n",
    "    # This is our final model:\n",
    "    vqa_model = Model(inputs=[cnn_inputs, filename_inputs], outputs=output)\n",
    "\n",
    "    vqa_model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    vqa_model.summary()\n",
    "    \n",
    "    history = vqa_model.fit([X_train_cnn, X_train_rnn], y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss',\n",
    "                                                 patience=3, \n",
    "                                                 min_delta=0.0001)])\n",
    "    plot_model(vqa_model, to_file='model.png')\n",
    "\n",
    "    print(\"###################################################################\")\n",
    "    print(\"\\nResults:\\n\")\n",
    "    accr = vqa_model.evaluate([X_test_cnn, X_test_rnn], y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],\n",
    "                                                                  accr[1]))\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Make predictions \n",
    "    y_pred = vqa_model.predict([X_test_cnn, X_test_rnn])\n",
    "    report = classification_report(y_test, y_pred.round(), target_names=my_tags)\n",
    "    print(\"\\nClassfication Report for test:\\n\", report)\n",
    "    print(\"###################################################################\")\n",
    "    return(vqa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94zQPusHYP4f"
   },
   "source": [
    "---\n",
    "### Build Reference Model (KNN)\n",
    "\n",
    "The reference model is a k-nearest-neighbours model. \n",
    "\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbtsAb3DexFu"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# KNN Reference model\n",
    "# ==============================================================================\n",
    "\n",
    "def knn(X_train, X_test, y_train ,y_test, my_tags):\n",
    "    '''\n",
    "    This is a reference model for the dissertation-standardisation task\n",
    "    The dimensionality for the feature set currently is (1716, 4096)\n",
    "    Where each row represents an organ of which has been reshaped into a 1d V\n",
    "    Performance will be compared to a 3D convolutional neural network\n",
    "    '''\n",
    "    print(\"\\n####################################################################\")\n",
    "    print(\"Building KNN Reference Model\")\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], 4096)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 4096)\n",
    "    # Dependencies ------------------------------------------------------------\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # Build model -------------------------------------------------------------\n",
    "    print(\"Building model...\")\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors = 18)\n",
    "    print(\"Training model...\")\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Evaluation --------------------------------------------------------------\n",
    "    print(\"Evaluation -------------------------------------------------------\")\n",
    "    print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "    #Classification report ----------------------------------------------------\n",
    "    report = classification_report(y_test, y_pred.round(),\\\n",
    "                                   target_names=my_tags)\n",
    "    print(\"\\nClassfication Report for test:\\n\", report)\n",
    "    print(\"\\n####################################################################\")\n",
    "    return(knn, report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIw9QkOr8y3K"
   },
   "source": [
    "#### PointNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5JZ1SdL82ks"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "from keras.layers import Dense, MaxPooling1D, Convolution1D, Dropout, Flatten, BatchNormalization, Reshape, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "def mat_mul(A, B):\n",
    "    return tf.matmul(A, B)\n",
    "\n",
    "    # Rotate and jitter points\n",
    "def rotate_point_cloud(batch_data):\n",
    "    \"\"\" Randomly rotate the point clouds to augument the dataset\n",
    "        rotation is per shape based along up direction\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, rotated batch of point clouds\n",
    "    \"\"\"\n",
    "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
    "    for k in range(batch_data.shape[0]):\n",
    "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
    "        cosval = np.cos(rotation_angle)\n",
    "        sinval = np.sin(rotation_angle)\n",
    "        rotation_matrix = np.array([[cosval, 0, sinval],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-sinval, 0, cosval]])\n",
    "        shape_pc = batch_data[k, ...]\n",
    "        rotated_data[k, ...] = np.dot(shape_pc.reshape((-1, 3)), rotation_matrix)\n",
    "    return rotated_data\n",
    "\n",
    "\n",
    "def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):\n",
    "    \"\"\" Randomly jitter points. jittering is per point.\n",
    "        Input:\n",
    "          BxNx3 array, original batch of point clouds\n",
    "        Return:\n",
    "          BxNx3 array, jittered batch of point clouds\n",
    "    \"\"\"\n",
    "    B, N, C = batch_data.shape\n",
    "    assert(clip > 0)\n",
    "    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1 * clip, clip)\n",
    "    jittered_data += batch_data\n",
    "    return jittered_data\n",
    "\n",
    "    # ==========================================================================\n",
    "    # PointNet Full Model\n",
    "    # ==========================================================================\t\n",
    "def pointnet_full(y, my_tags, test_ids, num_classes=18):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load data\n",
    "    desired_points = 1024\n",
    "    X = np.load('data/processed/pointnetdata.npy', allow_pickle=True)\n",
    "    \n",
    "    X_train = X[test_ids[:-int(np.ceil(X.shape[0]*0.25))]]\n",
    "    X_test = X[test_ids[-int(np.ceil(X.shape[0]*0.25)):]]\n",
    "    y_train = y[test_ids[:-int(np.ceil(y.shape[0]*0.25))]]\n",
    "    y_test = y[test_ids[-int(np.ceil(X.shape[0]*0.25)):]]\n",
    "\n",
    "    # Training set\n",
    "    train_points_r = X_train\n",
    "    train_labels_r = y_train\n",
    "\n",
    "    # Test set\n",
    "    test_points_r = X_test\n",
    "    test_labels_r = y_test\n",
    "\n",
    "    # label to categorical\n",
    "    from keras.utils import to_categorical\n",
    "    #y_test = to_categorical(y_test)\n",
    "    #y_train = to_categorical(y_train)\n",
    "    # Let's examine the data. \n",
    "\n",
    "    print(\"Training shape: \", train_points_r.shape)\n",
    "    print(\"Test shape: \\t\", test_points_r.shape)\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # hyperparameter\n",
    "    # number of points in each sample\n",
    "    num_points = desired_points\n",
    "\n",
    "    # number of categories\n",
    "    k = 18\n",
    "\n",
    "    # define optimizer\n",
    "    opt = Adam(lr=0.001, decay=0.7)\n",
    "\n",
    "    max_epochs=25\n",
    "    batch_size=32\n",
    "    dropout_rate = 0.7\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    ### POINTNET ARCHITECTURE\n",
    "\n",
    "    input_points = Input(shape=(num_points, 3))\n",
    "    x = Convolution1D(64, 1, activation='relu', input_shape=(num_points, 3))(input_points)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution1D(128, 1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution1D(1024, 1, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=num_points)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
    "    input_T = Reshape((3, 3))(x)\n",
    "\n",
    "    # For affine transformation need to matrix multiply\n",
    "    # forward net\n",
    "    g = Lambda(mat_mul, arguments={'B': input_T})(input_points)\n",
    "    g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Convolution1D(64, 1, input_shape=(num_points, 3), activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "\n",
    "    # feature transform net\n",
    "    f = Convolution1D(64, 1, activation='relu')(g)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Convolution1D(128, 1, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Convolution1D(1024, 1, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = MaxPooling1D(pool_size=num_points)(f)\n",
    "    f = Dense(512, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(256, activation='relu')(f)\n",
    "    f = BatchNormalization()(f)\n",
    "    f = Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
    "    feature_T = Reshape((64, 64))(f)\n",
    "\n",
    "\n",
    "    # forward net\n",
    "    g = Lambda(mat_mul, arguments={'B': feature_T})(g)\n",
    "    g = Convolution1D(64, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Convolution1D(128, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Convolution1D(1024, 1, activation='relu')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "\n",
    "\n",
    "    # global_feature\n",
    "    global_feature = MaxPooling1D(pool_size=num_points)(g)\n",
    "\n",
    "\n",
    "    # point_net_cls\n",
    "    c = Dense(512, activation='relu')(global_feature)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dropout(rate=dropout_rate)(c)\n",
    "    c = Dense(256, activation='relu')(c)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Dropout(rate=dropout_rate)(c)\n",
    "    c = Dense(num_classes, activation='sigmoid')(c)\n",
    "    prediction = Flatten()(c)\n",
    "\n",
    "\n",
    "    # print the model summary\n",
    "    model = Model(inputs=input_points, outputs=prediction)\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    # compile classification model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Fit model on training data\n",
    "    for i in range(1,max_epochs+1):\n",
    "        # model.fit(train_points_r, Y_train, batch_size=32, epochs=1, shuffle=True, verbose=1)\n",
    "        # rotate and jitter the points\n",
    "        train_points_rotate = rotate_point_cloud(train_points_r)\n",
    "        train_points_jitter = jitter_point_cloud(train_points_rotate)\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=1,\\\n",
    "                        shuffle=True, verbose=0, validation_split=0.1)\n",
    "        s = \"Current epoch is:\" + str(i)\n",
    "        print(s)\n",
    "        if i % 5 == 0:\n",
    "            score = model.evaluate(test_points_r, y_test, verbose=1)\n",
    "            print('Test loss: ', score[0])\n",
    "            print('Test accuracy: ', score[1])\n",
    "\n",
    "\n",
    "    # ## 10. Evaluate the Model\n",
    "    # score the model\n",
    "    score = model.evaluate(test_points_r, y_test, verbose=1)\n",
    "    print('Test loss: ', score[0])\n",
    "    print('Test accuracy: ', score[1])\n",
    "\n",
    "    print(\"###################################################################\")\n",
    "    print(\"\\nResults:\\n\")\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Classification Report\n",
    "\n",
    "    # make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    ################################################################################\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    print(\"\\n###################### Model Performance ############################\")\n",
    "    # evaluate the model\n",
    "    _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('\\nTrain: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "    ################################################################################\n",
    "    print(\"\\n#####################################################################\")\n",
    "\n",
    "    #Classification report\n",
    "    report = classification_report(y_test, y_pred.round(), target_names=my_tags)\n",
    "    print(\"\\nClassfication Report for test:\\n\", report)\n",
    "    print(\"\\n#####################################################################\")\n",
    "\n",
    "    return(model, report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIsk-lGPdU3Y"
   },
   "source": [
    "### Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 174849,
     "status": "ok",
     "timestamp": 1564305129390,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "8CNOYCvGbD0q",
    "outputId": "66d9a225-ecd0-4c47-ec7f-dac85aa43da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################\n",
      "Building KNN Reference Model\n",
      "Building model...\n",
      "Training model...\n",
      "Evaluation -------------------------------------------------------\n",
      "Test set score: 0.46\n",
      "\n",
      "Classfication Report for test:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Apex       0.00      0.00      0.00         2\n",
      "           Bladder       0.64      0.23      0.34        78\n",
      "     Bladder noisy       0.00      0.00      0.00        74\n",
      "              Body       1.00      0.89      0.94        65\n",
      "               CTV       0.24      0.11      0.15       271\n",
      "             Crura       0.00      0.00      0.00        12\n",
      "      Femoral head       1.00      0.83      0.91        58\n",
      " Femoral head left       0.97      0.81      0.88        75\n",
      "Femoral head right       0.98      0.86      0.92        76\n",
      "               GTV       0.00      0.00      0.00        32\n",
      "          Hydrogel       0.00      0.00      0.00         6\n",
      "             Other       0.46      0.72      0.56       607\n",
      "          Prostate       0.48      0.09      0.14       129\n",
      "    Prostate noisy       0.00      0.00      0.00        54\n",
      "            Rectum       0.00      0.00      0.00       100\n",
      "      Rectum noisy       0.62      0.11      0.18       147\n",
      "        Tolerance1       0.91      1.00      0.95        60\n",
      "        Tolerance2       0.95      1.00      0.98        79\n",
      "\n",
      "         micro avg       0.58      0.46      0.51      1925\n",
      "         macro avg       0.46      0.37      0.39      1925\n",
      "      weighted avg       0.49      0.46      0.44      1925\n",
      "       samples avg       0.46      0.46      0.46      1925\n",
      "\n",
      "\n",
      "####################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "knn, knn_report = knn(X_train_cnn, X_test_cnn, y_train, y_test, my_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7Qxxrwniv4b"
   },
   "source": [
    "#### PointNet Model\n",
    "\n",
    "PointNet takes a non-Euclidean approach to 3D data classification and takes cartesian coordinates (x,y,z) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 973064,
     "status": "error",
     "timestamp": 1564227575876,
     "user": {
      "displayName": "Phillip Hungerford",
      "photoUrl": "https://lh3.googleusercontent.com/-nSVU3pfDKvc/AAAAAAAAAAI/AAAAAAAA7nM/C4RLaD0imSQ/s64/photo.jpg",
      "userId": "04339632095681498586"
     },
     "user_tz": -600
    },
    "id": "tNlH_0ttHEMS",
    "outputId": "9444fda1-6059-487b-f79e-33b721e9dd16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (5773, 1024, 3)\n",
      "Test shape: \t (1925, 1024, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 09:12:10.893286 139756220352384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0728 09:12:10.894582 139756220352384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0728 09:12:10.902050 139756220352384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0728 09:12:11.018502 139756220352384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0728 09:12:11.221660 139756220352384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0728 09:12:11.426316 139756220352384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0728 09:12:16.155474 139756220352384 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0728 09:12:16.156538 139756220352384 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0728 09:12:16.275402 139756220352384 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0728 09:12:16.324631 139756220352384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 09:12:16.348863 139756220352384 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1024, 3)           0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 1024, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1024, 64)          256       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1024, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1024, 64)          256       \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 1024, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1024, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 1024, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1024, 128)         8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 1024, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 1024, 1024)        132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1024, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 1, 512)            2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1, 256)            131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 1, 256)            1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1, 18)             4626      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18)                0         \n",
      "=================================================================\n",
      "Total params: 818,194\n",
      "Trainable params: 813,970\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n",
      "None\n",
      "Current epoch is:1\n",
      "Current epoch is:2\n",
      "Current epoch is:3\n",
      "Current epoch is:4\n",
      "Current epoch is:5\n",
      "1925/1925 [==============================] - 4s 2ms/step\n",
      "Test loss:  0.1234674020866295\n",
      "Test accuracy:  0.950562735780493\n",
      "Current epoch is:6\n",
      "Current epoch is:7\n",
      "Current epoch is:8\n",
      "Current epoch is:9\n",
      "Current epoch is:10\n",
      "1925/1925 [==============================] - 4s 2ms/step\n",
      "Test loss:  0.10221430961574828\n",
      "Test accuracy:  0.9546320060011628\n",
      "Current epoch is:11\n",
      "Current epoch is:12\n",
      "Current epoch is:13\n",
      "Current epoch is:14\n",
      "Current epoch is:15\n",
      "1925/1925 [==============================] - 4s 2ms/step\n",
      "Test loss:  0.09571684184399518\n",
      "Test accuracy:  0.9569119577593618\n",
      "Current epoch is:16\n",
      "Current epoch is:17\n",
      "Current epoch is:18\n",
      "Current epoch is:19\n",
      "Current epoch is:20\n",
      "1925/1925 [==============================] - 4s 2ms/step\n",
      "Test loss:  0.09554223816890221\n",
      "Test accuracy:  0.9574891577757798\n"
     ]
    }
   ],
   "source": [
    "pointnet, pointnet_report = pointnet_full(y, my_tags, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txDnUv_cjeZx"
   },
   "source": [
    "#### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83VcefDUJ7ny"
   },
   "outputs": [],
   "source": [
    "cnn, cnn_report = CNN(X_train_cnn, X_test_cnn, y_train, y_test, k, my_tags = my_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AN6694AWji2X"
   },
   "source": [
    "#### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNE1IwfQLegY"
   },
   "outputs": [],
   "source": [
    "rnn = rnn(X_train_rnn, X_test_rnn, y_train, y_test, k, my_tags=my_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DusSyZ9tjk87"
   },
   "source": [
    "#### Our model (3DCNN + RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CI-aqLBdLikl"
   },
   "outputs": [],
   "source": [
    "final_model = standardisation_model(X_train_cnn, X_train_rnn, X_test_cnn, X_test_rnn, y_train, y_test, k, my_tags=my_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8VjaarJl2Qk"
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02-models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
